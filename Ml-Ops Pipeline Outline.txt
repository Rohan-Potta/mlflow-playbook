Ml-Ops Pipeline:
	1) They need a pipeline to retrain models , the point is so that the if the client has new data they can re-train the models
	2) Create an end-to-end pipeline for data(data-ingestion, data-preprocessing, data-correlation, data-versioning and ML storing the artifacts and parameters)
	3) Model surveying and directly deploying it into the env (dev , staging and production)



1) PCAP analysis 
	we have 7 models 1 model for a single protocol 
	it is stored in an NFS 
	we perform pre-processing on the pcap and make a filter to get the features we require
	
	there should be a versioning for pre-processing data 
	this data is used to train the models
	once model-training is complete we have post-processing 
	then we pass it to model-registry to maintain the versioning of the models
	 

2) CDR Analysis
	CDR is present in the NFS storage 
	The pipeline will also have the pre-processing->anaomly-detection-model -> Post-Processing (for both TAS and SBG needs different pipelines)


Note:
In DVC we only store the last 2-5 versions of the data
In the ML-flow UI they want to see the params and the performance metrics
The pipeline should also be designed so that the newly trained model only if that accuracy is more than already existing model accuracy should it be deployed


Things to decide:
How to trigger the pipelines (Currently We do a manual Trigger)